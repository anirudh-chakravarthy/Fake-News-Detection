{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2104,
     "status": "ok",
     "timestamp": 1565029159314,
     "user": {
      "displayName": "ANIRUDH SRINIVASAN CHAKRAVARTHY",
      "photoUrl": "",
      "userId": "15662816526463818185"
     },
     "user_tz": -330
    },
    "id": "9pGOAF402foq",
    "outputId": "f607e955-80a7-471c-efbf-0a1797c8aaf8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd drive/My\\ Drive/NUS-Fake-News-Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1z2vdlor2ZN"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Bidirectional, CuDNNGRU, CuDNNLSTM, Activation,\\\n",
    "                        Dense, Input, concatenate, Dropout, GlobalMaxPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "\n",
    "# random seed for reproducibility\n",
    "np.random.seed(4123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-cAkmNEsPPK"
   },
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8308,
     "status": "ok",
     "timestamp": 1565029166063,
     "user": {
      "displayName": "ANIRUDH SRINIVASAN CHAKRAVARTHY",
      "photoUrl": "",
      "userId": "15662816526463818185"
     },
     "user_tz": -330
    },
    "id": "EI_ckpG3snCf",
    "outputId": "3e36d5ba-e88c-498b-8be4-0bddf21bf8e6"
   },
   "outputs": [],
   "source": [
    "!ls dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9O-26dZsOil"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset/train2.tsv', sep='\\t')\n",
    "val_df = pd.read_csv('dataset/val2.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('dataset/test2.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4VRDiuEgtpFt"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTSzsdYfwfg6"
   },
   "source": [
    "## Creating embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19050,
     "status": "ok",
     "timestamp": 1565029177430,
     "user": {
      "displayName": "ANIRUDH SRINIVASAN CHAKRAVARTHY",
      "photoUrl": "",
      "userId": "15662816526463818185"
     },
     "user_tz": -330
    },
    "id": "EZzhOjZ0we6B",
    "outputId": "7532dbf3-5c04-490f-842a-661b720b0281"
   },
   "outputs": [],
   "source": [
    "def readGloveFile(gloveFile):\n",
    "    with open(gloveFile, 'r') as f:\n",
    "        wordToGlove = {}  # map from a token (word) to a Glove embedding vector\n",
    "        wordToIndex = {}  # map from a token to an index\n",
    "        indexToWord = {}  # map from an index to a token \n",
    "\n",
    "        for line in f:\n",
    "            record = line.strip().split()\n",
    "            token = record[0] # take the token (word) from the text line\n",
    "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) # associate the Glove embedding vector to a that token (word)\n",
    "\n",
    "        tokens = sorted(wordToGlove.keys())\n",
    "        for idx, tok in enumerate(tokens):\n",
    "            kerasIdx = idx + 1\n",
    "            wordToIndex[tok] = kerasIdx # associate an index to a token (word)\n",
    "            indexToWord[kerasIdx] = tok # associate a word to a token (word). Note: inverse of dictionary above\n",
    "\n",
    "    return wordToIndex, indexToWord, wordToGlove\n",
    "\n",
    "# Create Pretrained Keras Embedding Layer\n",
    "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable, inputLayer=None):\n",
    "    vocabLen = len(wordToIndex) + 1  # adding 1 to account for masking\n",
    "    embDim = next(iter(wordToGlove.values())).shape[0]\n",
    "\n",
    "    embeddingMatrix = np.zeros((vocabLen, embDim))  # initialize with zeros\n",
    "    for word, index in wordToIndex.items():\n",
    "        embeddingMatrix[index, :] = wordToGlove[word] # create embedding: word index to Glove word embedding\n",
    "\n",
    "    if inputLayer is None:\n",
    "        embeddingLayer = Embedding(vocabLen, embDim, weights=[embeddingMatrix], trainable=isTrainable)\n",
    "    else:\n",
    "        embeddingLayer = Embedding(vocabLen, embDim, weights=[embeddingMatrix], trainable=isTrainable) (inputLayer)\n",
    "    return embeddingLayer\n",
    "\n",
    "wordToIndex, indexToWord, wordToGlove = readGloveFile('glove.6B.100d.txt')\n",
    "pretrainedEmbeddingLayer = createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ADHziK_hw3P4"
   },
   "source": [
    "## Loading relevant data corresponding to the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHg5qltjtlFP"
   },
   "outputs": [],
   "source": [
    "# mapping labels to integers\n",
    "def process_labels(labels, classifier):\n",
    "    if classifier == 'binary':\n",
    "        labels = labels.replace({'half-true': 1, 'mostly-true': 1, 'true': 1, \n",
    "                                 'barely-true': 0, 'pants-fire': 0, 'false': 0})\n",
    "    else:\n",
    "        labels = labels.replace({'pants-fire': 0, 'false': 1, 'barely-true': 2, \n",
    "                                 'half-true': 3, 'mostly-true': 4, 'true': 5})\n",
    "    return labels  \n",
    "\n",
    "\n",
    "def s_condition(df, classifier='sixway'):\n",
    "    labels = df.iloc[:, 2]\n",
    "    labels = process_labels(labels, classifier)\n",
    "    \n",
    "    data = df.iloc[:, 3].tolist()\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(data)\n",
    "    tokens = t.texts_to_sequences(data)\n",
    "    data = pad_sequences(tokens)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def sj_condition(df, classifier='sixway'):\n",
    "    labels = df.iloc[:, 2]\n",
    "    labels = process_labels(labels, classifier)\n",
    "    \n",
    "    statement = df.iloc[:, 3]\n",
    "    justification = df.iloc[:, 15]\n",
    "    data = (statement.map(str) + justification.map(str)).tolist()\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(data)\n",
    "    tokens = t.texts_to_sequences(data)\n",
    "    data = pad_sequences(tokens)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def sjplus_condition(df, classifier='sixway'):\n",
    "    labels = df.iloc[:, 2]\n",
    "    labels = process_labels(labels, classifier)\n",
    "    \n",
    "    statement = df.iloc[:, 3]\n",
    "    statement = statement.map(str).tolist()\n",
    "    justification = df.iloc[:, 15]\n",
    "    justification = justification.map(str).tolist()\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(statement + justification)\n",
    "    tokens = t.texts_to_sequences(statement)\n",
    "    statement = pad_sequences(tokens, maxlen=800)\n",
    "    tokens = t.texts_to_sequences(justification)\n",
    "    justification = pad_sequences(tokens, maxlen=800)\n",
    "\n",
    "    data = [statement, justification]\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jS1NrvXew7yg"
   },
   "source": [
    "## Creating models based for each conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgg0wEd8vnIl"
   },
   "outputs": [],
   "source": [
    "def s_model(classifier='sixway'):\n",
    "    model = Sequential()\n",
    "    model.add(pretrainedEmbeddingLayer)\n",
    "    model.add(Bidirectional(CuDNNSTM(32)))\n",
    "    if classifier == 'binary':\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(Dense(6, activation='softmax'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def sj_model(classifier='sixway'):\n",
    "    model = Sequential()\n",
    "    model.add(pretrainedEmbeddingLayer)\n",
    "    model.add(Bidirectional(CuDNNLSTM(32)))\n",
    "    \n",
    "    if classifier == 'binary':\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(Dense(6, activation='softmax'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def sjplus_model(input_dims, classifier='sixway'):\n",
    "    input1 = Input(shape=(input_dims[0],))\n",
    "    x1 = createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, False, input1)\n",
    "    x1 = Dropout(0.1) (x1)\n",
    "    x1 = Bidirectional(CuDNNGRU(32, return_sequences=True)) (x1)\n",
    "    x1 = Dropout(0.1) (x1)\n",
    "    x1 = Bidirectional(CuDNNLSTM(32, return_sequences=True)) (x1)\n",
    "    x1 = Dropout(0.1) (x1)\n",
    "    x1 = GlobalMaxPool1D()(x1)\n",
    "    x1 = Dense(50, activation=\"relu\")(x1)\n",
    "    x1 = Dropout(0.4)(x1)\n",
    "    model1 = Model(inputs=input1, outputs=x1)\n",
    "\n",
    "    input2 = Input(shape=(input_dims[1],))\n",
    "    x2 = createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, False, input2)\n",
    "    x2 = Dropout(0.1)(x2)\n",
    "    x2 = Bidirectional(CuDNNGRU(32, return_sequences=True)) (x2)\n",
    "    x2 = Dropout(0.1)(x2)\n",
    "    x2 = Bidirectional(CuDNNLSTM(32, return_sequences=True,)) (x2)\n",
    "    x2 = Dropout(0.1)(x2)\n",
    "    x2 = GlobalMaxPool1D()(x2)\n",
    "    x2 = Dense(50, activation=\"relu\")(x2)\n",
    "    x2 = Dropout(0.4)(x2)\n",
    "    model2 = Model(inputs=input2, outputs=x2)\n",
    "    \n",
    "    x = concatenate([model1.output, model2.output])\n",
    "    if classifier == 'binary':\n",
    "        out = Dense(1, activation='sigmoid') (x)\n",
    "    else:\n",
    "        out = Dense(6, activation='softmax') (x)\n",
    "    model = Model(inputs=[model1.input, model2.input], outputs=out)\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7HEYL9D24or"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FacQYRt88QJ7"
   },
   "outputs": [],
   "source": [
    "ckpt_path = 'fake_news_binary.hdf5'\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_acc', patience=5, verbose=1, restore_best_weights=True)\n",
    "reducelr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, verbose=1, min_lr=1.e-6)\n",
    "modelckpt_cb = ModelCheckpoint(ckpt_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# tb = TensorBoard()\n",
    "\n",
    "callbacks = [earlystop, reducelr, modelckpt_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33720,
     "status": "error",
     "timestamp": 1565029193300,
     "user": {
      "displayName": "ANIRUDH SRINIVASAN CHAKRAVARTHY",
      "photoUrl": "",
      "userId": "15662816526463818185"
     },
     "user_tz": -330
    },
    "id": "bNcZRyuc24TO",
    "outputId": "4c04ff00-d658-4a1c-bf48-365624e6091e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier = 'binary'\n",
    "batch_size = 16\n",
    "np.random.seed(4123)\n",
    "\n",
    "x_train, y_train = sjplus_condition(train_df, classifier)\n",
    "x_val, y_val = sjplus_condition(val_df, classifier)\n",
    "\n",
    "binary_model = sjplus_model([len(x_train[0][0]), len(x_train[1][0])], classifier)\n",
    "binary_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1.e-5), metrics=['acc'])\n",
    "binary_history = binary_model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eChg7y2Wzt0G"
   },
   "outputs": [],
   "source": [
    "binary_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=3.e-4), metrics=['acc'])\n",
    "binary_history = binary_model.fit(x_train, y_train, validation_data=(x_val, y_val), \n",
    "                                  batch_size=batch_size, callbacks=callbacks, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPFyfDBvAs6R"
   },
   "source": [
    "# Model evalulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13216,
     "status": "ok",
     "timestamp": 1565029208356,
     "user": {
      "displayName": "ANIRUDH SRINIVASAN CHAKRAVARTHY",
      "photoUrl": "",
      "userId": "15662816526463818185"
     },
     "user_tz": -330
    },
    "id": "A5siWQ3n2eyw",
    "outputId": "d1a87582-f4a9-42c9-88f9-d9b5a5b8ee55"
   },
   "outputs": [],
   "source": [
    "x_test, y_test = sjplus_condition(test_df, classifier)\n",
    "\n",
    "binary_model.load_weights('fake_news_binary_58379.hdf5')\n",
    "score, acc = binary_model.evaluate(x_test, y_test)\n",
    "print('Test accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CEKLe7a1v_zr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Fake News Binary Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
